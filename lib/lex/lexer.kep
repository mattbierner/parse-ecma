/**
 * @fileOverview Lexers for ECMAScript 5.1.
 */
package (
    lexer
    
    lexStream
    lex)
with
    import 'bennu::parse' parse#{
        always
        attempt
        binds
        bind
        choice
        eof
        getPosition
        modifyState
        getState
        enumeration
        extract
        next
        many
        runState
        never
        ParserState},
    import 'bennu::lang' {then},
    import 'nu-stream::stream' {'from': streamFrom},
    
    import 'ecma-ast::token' lexToken,
    import 'ecma-ast::position' {SourceLocation SourcePosition},
    
    import './boolean_lexer' {booleanLiteral},
    import './comment_lexer' comment_lexer,
    import './identifier_lexer' {identifier},
    import './line_terminator_lexer' line_terminator_lexer,
    import './null_lexer' {nullLiteral},
    import './number_lexer' {numericLiteral},
    import './punctuator_lexer' {punctuator divPunctuator},
    import './reserved_word_lexer' {reservedWord},
    import './string_lexer' {stringLiteral},
    import './whitespace_lexer' whitespace_lexer,
    import './regular_expression_lexer' {regularExpressionLiteral}
in {

/* State
 ******************************************************************************/
/**
 * Update the previous token `self` when `tok` is consumed.
 */
var consume := \tok self -> {
    switch (tok.type) {
    case 'Comment':
    case 'Whitespace':
    case 'LineTerminator':
        return self;
    default:
        return tok;
    }
};

var isRegExpCtx := \ prev -> {
    if (!prev) return true;
    switch (prev.type) {
    case 'Keyword':
    case 'Punctuator':
        return true;
    
    }
    return false;
};

/* Token
 ******************************************************************************/
var makeToken = \type p ->
    p.map \ x -> [type, x];

var buildToken = \p ->
    binds(
        enumeration(
            getPosition,
            p,
            getPosition),
        \start [type value] end ->
            always(
                new type(
                    new SourceLocation(start, end, start.file || end.file),
                    value)))
    .chain \tok ->
        next(
            modifyState (consume @ tok),
            always tok);

var enterRegExpCtx := getState.chain \ prev ->
    ?isRegExpCtx prev
        :always()
        :never();

/* Lexers
 ******************************************************************************/
/**
 * Language literal.
 */
var literal = choice(
    makeToken(
        lexToken.StringToken.create,
        stringLiteral),
    makeToken(
        lexToken.BooleanToken.create,
        booleanLiteral),
    makeToken(
        lexToken.NullToken.create,
        nullLiteral),
    makeToken(
        lexToken.NumberToken.create,
        numericLiteral),
    makeToken(
        lexToken.RegularExpressionToken.create,
        next(
            enterRegExpCtx,
            regularExpressionLiteral)));

/**
 * Language token.
 */
var token := choice(
    attempt makeToken(
        lexToken.IdentifierToken,
        identifier),
    literal,
    makeToken(
        lexToken.KeywordToken,
        reservedWord),
    makeToken(
        lexToken.PunctuatorToken,
        punctuator));

/**
 * Lexer for a top level element in division contexts.
 */
var inputElement := choice(
    makeToken(
        lexToken.CommentToken,
        comment_lexer.comment),
    makeToken(
        lexToken.WhitespaceToken,
        whitespace_lexer.whitespace),
    makeToken(
        lexToken.LineTerminatorToken,
        line_terminator_lexer.lineTerminator),
    token);

/**
 * Tokenizer.
 */
lexer :=
    then(
        many buildToken(inputElement),
        eof);

/* Running
 ******************************************************************************/
/**
 * Lexes stream of characters
 */
lexStream = \s file -> 
    runState(
        lexer,
        new ParserState(
            s,
            new SourcePosition(1, 0, file),
            null));

/**
 * Lexes array-like input of characters.
 */
lex = streamFrom \> lexStream;

}